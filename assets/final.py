# -*- coding: utf-8 -*-
"""FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14IkN9Yb7GpDaMXMd_d6lldZ75wYn7wwU
"""
"""
!pip install cvlib
!pip install python-dotenv
!pip install mtcnn
!pip install mxnet
"""

from matplotlib import pyplot
import matplotlib.pyplot as plt
import cv2
from PIL import Image, ImageFilter
import numpy as np

"""
from google.colab import drive
drive.mount('/content/drive')
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/SENIOR DESIGN PROJECT

picture1 = sys.argv[1] #buraya file path
print('sa')

def boundary_box(filename, detected_faces):
    # load the image
    data = pyplot.imread(filename)
    # plot the image
    pyplot.imshow(data)
    # get the context for drawing boxes
    ax = pyplot.gca()
    # plot each box
    for face in detected_faces:
        # get coordinates
        x, y, width, height = face['box']
        # create the shape
        rect = Rectangle((x, y), width, height, fill=False, color='red')
        # draw the box
        ax.add_patch(rect)
    # show the plot
    pyplot.show()

def getFaceBox(net, image, conf_threshold=0.7):
    image=image.copy()
    imageHeight=image.shape[0]
    imageWidth=image.shape[1]
    blob=cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], True, False)
    net.setInput(blob)
    detections=net.forward()
    faceBoxes=[]
    for i in range(detections.shape[2]):
        confidence=detections[0,0,i,2]
        if confidence>conf_threshold:
            x1=int(detections[0,0,i,3]*imageWidth)
            y1=int(detections[0,0,i,4]*imageHeight)
            x2=int(detections[0,0,i,5]*imageWidth)
            y2=int(detections[0,0,i,6]*imageHeight)
            faceBoxes.append([x1,y1,x2,y2])
            cv2.rectangle(image, (x1,y1), (x2,y2), (0,255,0), int(round(imageHeight/150)), 8)
    return image,faceBoxes

# to display background and foreground faces - takes normal arrays instead of rects
def bf_boundary_box(filename, foreground_faces, background_faces):
    # load the image
    data = pyplot.imread(filename)
    # plot the image
    pyplot.imshow(data)
    # get the context for drawing boxes
    ax = pyplot.gca()
    # plot each box
    for face in foreground_faces:
        # get coordinates
        x, y, width, height, _, _ = face
        # create the shape
        rect = Rectangle((x, y), width, height, fill=False, color='green')
        # draw the box
        ax.add_patch(rect)

    for face in background_faces:
        # get coordinates
        x, y, width, height, _, _ = face
        # create the shape
        rect = Rectangle((x, y), width, height, fill=False, color='red')
        # draw the box
        ax.add_patch(rect)
    # show the plot
    pyplot.show()

def genderAge(genderNet, ageNet, blob):
  global i
  ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']
  genderList=['Male','Female']
  
  # Predict the gender
  genderNet.setInput(blob)
  genderPreds=genderNet.forward()
  gender=genderList[genderPreds[0].argmax()]
  # Predict the age
  ageNet.setInput(blob)
  agePreds=ageNet.forward()
  age=ageList[agePreds[0].argmax()]
  # Return

  return gender,age

def detect_faces(filename):
  faceProto = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\model\\facenet\\opencv_face_detector.pbtxt"
  faceModel = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\model\\facenet\\opencv_face_detector_uint8.pb"
  print('sa2')
  #Load face detection model
  faceNet=cv2.dnn.readNet(faceModel,faceProto)
  print('sa3')
  image = pyplot.imread(filename)
  resultImg,faceBoxes=getFaceBox(faceNet,image)

  return resultImg, faceBoxes

def classify_faces(filename, faceBoxes, resultImg):
  ageProto = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\model\\age\\age_deploy.prototxt"
  ageModel = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\model\\age\\age_net.caffemodel"
  genderProto = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\model\\gender\\gender_deploy.prototxt"
  genderModel = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\model\\gender\\gender_net.caffemodel"
  pathImg = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\Dataset/"
  APPROOT = "C:\\Users\\atkdo\\Desktop\\okul\\4 - 2\\CS 492\\desktop\\electron-react-app\\assets\\faceInformation\\"
  
  #Load age detection model
  ageNet=cv2.dnn.readNet(ageModel,ageProto)
  #Load gender detection model
  genderNet=cv2.dnn.readNet(genderModel,genderProto)
  
  image = pyplot.imread(filename)

  classified_faces = []

  # Loop through the faceboxes
  for faceBox in faceBoxes:
    # get facebox 
    MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)
    padding=20
    face=image[max(0,faceBox[1]-padding):
        min(faceBox[3]+padding,image.shape[0]-1),max(0,faceBox[0]-padding)
        :min(faceBox[2]+padding, image.shape[1]-1)]
    
    im = Image.fromarray(np.uint8(face))
    im.show()
    blurImage = im.filter(ImageFilter.BLUR)
    blurImage.show()
    blurImage.save('simBlurImage.jpg')

    blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)
    print("#====Detected Age and Gender====#")
    gender,age = genderAge(genderNet, ageNet, blob)
    print('Gender',gender)
    print('Age',age)
    classified_faces.append([gender, age])
    # Write images into the results directory
    cv2.imwrite(APPROOT+'results/'+str(image) + ".jpg", resultImg)
  plt.imshow(resultImg)
  return classified_faces

print('sa1')


resultImg, faceBoxes = detect_faces(picture1)

print('sa2')
if not faceBoxes:
  print("No face detected")

else:
  classified_faces = classify_faces(picture1, faceBoxes, resultImg)
  print(classified_faces)
  #blur_faces(picture1, faceBoxes)
  #for face in classified_faces:
    #generate_faces(gender = face[0], age = face[1])

print('sa')

