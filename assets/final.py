# -*- coding: utf-8 -*-
"""FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14IkN9Yb7GpDaMXMd_d6lldZ75wYn7wwU
"""

!pip install cvlib
!pip install python-dotenv
!pip install mtcnn
!pip install mxnet

import cv2
from matplotlib import pyplot
from matplotlib.patches import Rectangle
from mtcnn.mtcnn import MTCNN
from pathlib import Path
import logging
from dotenv import load_dotenv
import cv2
import sys
import glob
import logging
from PIL import Image, ImageFilter
import argparse
import numpy as np
import mxnet as mx
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import tensorflow as tf
from matplotlib import pyplot
from mtcnn.mtcnn import MTCNN
from PIL import Image
import pandas as pd
import os
from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Concatenate, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout,  GlobalAveragePooling2D, RepeatVector
from keras.models import Model, Sequential
from keras import backend as K
from keras.models import load_model
import os
import cv2
import argparse
import dlib
import numpy as np
import scipy.spatial as spatial
import logging
from google.colab.patches import cv2_imshow
from keras.utils.vis_utils import plot_model

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd drive/MyDrive/SENIOR DESIGN PROJECT

picture1 = "test_images/d.jpg"

def boundary_box(filename, detected_faces):
    # load the image
    data = pyplot.imread(filename)
    # plot the image
    pyplot.imshow(data)
    # get the context for drawing boxes
    ax = pyplot.gca()
    # plot each box
    for face in detected_faces:
        # get coordinates
        x, y, width, height = face['box']
        # create the shape
        rect = Rectangle((x, y), width, height, fill=False, color='red')
        # draw the box
        ax.add_patch(rect)
    # show the plot
    pyplot.show()

def face_detection_ex(filename):
  # load image from file
  pixels = pyplot.imread(filename)

  # create the detector, using default weights
  detector = MTCNN()
  # detect faces in the image
  faces = detector.detect_faces(pixels) 
  #faces contain for each face index:
  #box: the x, y, width, height of each detected face
  #confidence, key_points 

  # display faces on the original image
  boundary_box(filename, faces)

  return faces

def detect_background(filename):
  
  #todo: return two arrays, (x, y, width, height of foreground faces and x, y, width, height of background faces)
  print("todo")

#face_detection(picture1)

def getFaceBox(net, image, conf_threshold=0.7):
    image=image.copy()
    imageHeight=image.shape[0]
    imageWidth=image.shape[1]
    blob=cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], True, False)
    net.setInput(blob)
    detections=net.forward()
    faceBoxes=[]
    for i in range(detections.shape[2]):
        confidence=detections[0,0,i,2]
        if confidence>conf_threshold:
            x1=int(detections[0,0,i,3]*imageWidth)
            y1=int(detections[0,0,i,4]*imageHeight)
            x2=int(detections[0,0,i,5]*imageWidth)
            y2=int(detections[0,0,i,6]*imageHeight)
            faceBoxes.append([x1,y1,x2,y2])
            cv2.rectangle(image, (x1,y1), (x2,y2), (0,255,0), int(round(imageHeight/150)), 8)
    return image,faceBoxes

# to display background and foreground faces - takes normal arrays instead of rects
def bf_boundary_box(filename, foreground_faces, background_faces):
    # load the image
    data = pyplot.imread(filename)
    # plot the image
    pyplot.imshow(data)
    # get the context for drawing boxes
    ax = pyplot.gca()
    # plot each box
    for face in foreground_faces:
        # get coordinates
        x, y, width, height, _, _ = face
        # create the shape
        rect = Rectangle((x, y), width, height, fill=False, color='green')
        # draw the box
        ax.add_patch(rect)

    for face in background_faces:
        # get coordinates
        x, y, width, height, _, _ = face
        # create the shape
        rect = Rectangle((x, y), width, height, fill=False, color='red')
        # draw the box
        ax.add_patch(rect)
    # show the plot
    pyplot.show()

def face_detection_oq(filename):
  # load image from file
  pixels = pyplot.imread(filename)

  # create the detector, using default weights
  detector = MTCNN()
  # detect faces in the image
  faces = detector.detect_faces(pixels) 
  #faces contain for each face index:
  #box: the x, y, width, height of each detected face
  #confidence, key_points 

  # display faces on the original image
  boundary_box(filename, faces)

  return faces

def detect_background(filename):
  image = cv2.imread(filename)
  size = image.shape
  faces = face_detection_oq(filename)

  ratio_box_array = []
  for face in faces:
    # new array with_ratio = x, y, w, h, woran, horan (image and faces ratio)
    with_ratio = face['box'].copy()
    with_ratio.append(face['box'][2] / size[1])
    with_ratio.append(face['box'][3] / size[0])

    ratio_box_array.append(with_ratio)
    
  # sort accroding to ratio
  ratio_box_array.sort(key = lambda ratio_box_array: ratio_box_array[4], reverse=True)
  ratio_box_array = np.array(ratio_box_array)

  # compare ratios and find background ones

  max_ratio = ratio_box_array[0][4]
  print(max_ratio)

  # 4th index have the ratio of the width ratios to the max ratio
  ratio_box_array[:,4] /= max_ratio

  fore_array = []
  back_array = []

  for box in ratio_box_array:
    if max_ratio > 0.05:
      if box[4] > 0.8:
        fore_array.append(box)
      else:
        back_array.append(box)
    else:    
      if box[4] > 0.6:
        fore_array.append(box)
      else:
        back_array.append(box)

  # display foreground background faces 
  bf_boundary_box(filename, fore_array, back_array)

  #todo: return two arrays, (x, y, width, height of foreground faces and x, y, width, height of background faces)
  print("todo")

def genderAge(genderNet, ageNet, blob):
  global i
  ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']
  genderList=['Male','Female']
  
  # Predict the gender
  genderNet.setInput(blob)
  genderPreds=genderNet.forward()
  gender=genderList[genderPreds[0].argmax()]
  # Predict the age
  ageNet.setInput(blob)
  agePreds=ageNet.forward()
  age=ageList[agePreds[0].argmax()]
  # Return

  return gender,age

def detect_faces(filename):
  faceProto = "faceInformation/model/facenet/opencv_face_detector.pbtxt"
  faceModel = "faceInformation/model/facenet/opencv_face_detector_uint8.pb"
  
  #Load face detection model
  faceNet=cv2.dnn.readNet(faceModel,faceProto)

  image = pyplot.imread(filename)
  resultImg,faceBoxes=getFaceBox(faceNet,image)

  return resultImg, faceBoxes

def classify_faces(filename, faceBoxes, resultImg):
  ageProto = "faceInformation/model/age/age_deploy.prototxt"
  ageModel = "faceInformation/model/age/age_net.caffemodel"
  genderProto = "faceInformation/model/gender/gender_deploy.prototxt"
  genderModel = "faceInformation/model/gender/gender_net.caffemodel"
  pathImg = "faceInformation/Dataset/"
  APPROOT = "faceInformation/"
  
  #Load age detection model
  ageNet=cv2.dnn.readNet(ageModel,ageProto)
  #Load gender detection model
  genderNet=cv2.dnn.readNet(genderModel,genderProto)
  
  image = pyplot.imread(filename)

  classified_faces = []

  # Loop through the faceboxes
  for faceBox in faceBoxes:
    # get facebox 
    MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)
    padding=20
    face=image[max(0,faceBox[1]-padding):
        min(faceBox[3]+padding,image.shape[0]-1),max(0,faceBox[0]-padding)
        :min(faceBox[2]+padding, image.shape[1]-1)]
    
    im = Image.fromarray(np.uint8(face))
    im.show()
    blurImage = im.filter(ImageFilter.BLUR)
    blurImage.show()
    blurImage.save('simBlurImage.jpg')

    blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)
    print("#====Detected Age and Gender====#")
    gender,age = genderAge(genderNet, ageNet, blob)
    print('Gender',gender)
    print('Age',age)
    classified_faces.append([gender, age])
    # Write images into the results directory
    cv2.imwrite(APPROOT+'results/'+str(image) + ".jpg", resultImg)
  plt.imshow(resultImg)
  return classified_faces

def blur_faces(filename, faceBoxes):
  image = pyplot.imread(filename)
  # Loop through the faceboxes
  for faceBox in faceBoxes:
    # get facebox 
    MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)
    padding=20
    face=image[max(0,faceBox[1]-padding):
        min(faceBox[3]+padding,image.shape[0]-1),max(0,faceBox[0]-padding)
        :min(faceBox[2]+padding, image.shape[1]-1)]
      
    im = Image.fromarray(np.uint8(face))
    im.show()
    blurImage = im.filter(ImageFilter.BLUR)
    blurImage.show()
    blurImage.save('blurred.jpg')

# This function shows 25 images from a batch 

def show_plot(image_batch):
  # Define the onscreen size of the image
  plt.figure(figsize=(10, 10))

  # For each of 25 images from the batch
  for n in range(25):
    # Plot the image with pixel values scaled from [-1,1] to [0,255]
    ax = plt.subplot(5, 5, n + 1)
    plt.imshow(((image_batch[n] * 127.5) + 127.5) / 255)
    plt.axis('off')
  
  # Plot the images
  plt.show()

def generate_faces(gender = "Male", age = "(25-32)"):
  batch_size = 64
  input_img_size = (64,64,3)                                                        
  num_images = 18430 
  latent_dim = 100
  n_labels = 40

  generator = load_model('c_gan_0048.h5')
  attributes = np.full(shape=(64, n_labels), fill_value=0, dtype=np.int32)

  #@title Attributes
  Male = False 
  Female = True
  #Male = false, Female = True
  if gender == "Female":
    attributes[:,20] = int(Female)
  else:
    attributes[:,20] = int(Male)
  ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']
  # ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']

  Young = True
  Old = False
  if age in ageList[0:4]:
    attributes[:,39] = int(Young)
  else:
    attributes[:,39] = int(Old)

  Bald = False 
  attributes[:,4] = int(Bald)

  Receding_Hairline = False 
  attributes[:,28] = int(Receding_Hairline)

  Sideburns =  False 
  attributes[:,30] = int(Sideburns)

  Bangs = False 
  attributes[:,5] = int(Bangs)

  Black_Hair = False 
  attributes[:,8] = int(Black_Hair)

  Blonde_Hair = True 
  attributes[:,9] = int(Blonde_Hair)

  Brown_Hair = False 
  attributes[:,11] = int(Brown_Hair)

  Gray_Hair = False 
  attributes[:,17] = int(Gray_Hair)

  Straight_Hair = False 
  attributes[:,32] = int(Straight_Hair)

  Wavy_Hair = False 
  attributes[:,33] = int(Wavy_Hair)

  Narrow_Eyes = False 
  attributes[:,23] = int(Narrow_Eyes)

  Arched_Eyebrows = False
  attributes[:,1] = int(Arched_Eyebrows)

  Bushy_Eyebrows = False 
  attributes[:,12] = int(Bushy_Eyebrows)

  Eyeglasses = False
  attributes[:,15] = int(Eyeglasses)

  Bags_Under_Eyes = False
  attributes[:,3] = int(Bags_Under_Eyes)

  Chubby = False
  attributes[:,13] = int(Chubby)

  Double_Chin = False 
  attributes[:,14] = int(Double_Chin)

  High_Cheekbones = False 
  attributes[:,19] = int(High_Cheekbones)

  Oval_Face = False 
  attributes[:,25] = int(Oval_Face)

  Pale_Skin = True
  attributes[:,26] = int(Pale_Skin)

  Rosy_Cheeks = False
  attributes[:,29] = int(Rosy_Cheeks)

  Attractive = True
  attributes[:,2] = int(Attractive)

  Big_Nose = False 
  attributes[:,7] = int(Big_Nose)

  Pointy_Nose = False 
  attributes[:,27] = int(Pointy_Nose)

  Big_Lips = False 
  attributes[:,6] = int(Big_Lips)

  Mouth_Slightly_Open = False 
  attributes[:,21] = int(Mouth_Slightly_Open)

  Smiling = True 
  attributes[:,31] = int(Smiling)

  Goatee = False 
  attributes[:,16] = int(Goatee)

  Mustache = False
  attributes[:,22] = int(Mustache)

  No_Beard = False 
  attributes[:,24] = int(No_Beard)

  Heavy_Makeup = False 
  attributes[:,18] = int(Heavy_Makeup)

  Wearing_Lipstick = False 
  attributes[:,36] = int(Wearing_Lipstick)

  Wearing_Earrings = False 
  attributes[:,34] = int(Wearing_Earrings)

  Wearing_Hat = False 
  attributes[:,35] = int(Wearing_Hat)

  Wearing_Necklace = False
  attributes[:,37] = int(Wearing_Necklace)

  Wearing_Necktie = False
  attributes[:,38] = int(Wearing_Necktie)

  Five_o_Clock_Shadow = False
  attributes[:,0] = int(Five_o_Clock_Shadow)

  Blurry = False 
  attributes[:,10] = int(Blurry)

  sample_noise = input_gen(n = 64)
  sample_combination = [sample_noise, attributes]
  generated_sample = generator.predict(sample_combination)
  show_plot(generated_sample)

  plt.figure(figsize = (2.5,2.5))
  plt.imshow(((generated_sample[11] * 127.5) + 127.5) / 255)

#this function generates the latent vector in input to the generator along with randomly selected labels

def input_gen(latent_dim = 100, n = 100, n_classes=40):
	# generate points in the latent space
	x_input = np.random.randn(latent_dim * n)
	# reshape into a batch of inputs for the network
	z_input = x_input.reshape(n, latent_dim)
	# generate labels
	return [z_input]

## Face and points detection
def face_points_detection(predictor, img, bbox:dlib.rectangle):
    # Get the landmarks/parts for the face in box d.
    shape = predictor(img, bbox)

    # loop over the 68 facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    coords = np.asarray(list([p.x, p.y] for p in shape.parts()), dtype=np.int)

    # return the array of (x, y)-coordinates
    return coords

def select_face(predictor, im, r=10, choose=True):
    faces = face_detection_blend(im)

    if len(faces) == 0:
        return None, None, None

    if len(faces) == 1 or not choose:
        idx = np.argmax([(face.right() - face.left()) * (face.bottom() - face.top()) for face in faces])
        bbox = faces[idx]
    else:
        bbox = []

        def click_on_face(event, x, y, flags, params):
            if event != cv2.EVENT_LBUTTONDOWN:
                return

            for face in faces:
                if face.left() < x < face.right() and face.top() < y < face.bottom():
                    bbox.append(face)
                    break

        im_copy = im.copy()
        for face in faces:
            # draw the face bounding box
            cv2.rectangle(im_copy, (face.left(), face.top()), (face.right(), face.bottom()), (0, 0, 255), 1)
        cv2.imshow('Click the Face:', im_copy)
        cv2.setMouseCallback('Click the Face:', click_on_face)
        while len(bbox) == 0:
            cv2.waitKey(1)
        cv2.destroyAllWindows()
        bbox = bbox[0]

    points = np.asarray(face_points_detection(predictor, im, bbox))

    im_w, im_h = im.shape[:2]
    left, top = np.min(points, 0)
    right, bottom = np.max(points, 0)

    x, y = max(0, left - r), max(0, top - r)
    w, h = min(right + r, im_h) - x, min(bottom + r, im_w) - y

    return points - np.asarray([[x, y]]), (x, y, w, h), im[y:y + h, x:x + w]

def select_all_faces(predictor, im, r=10):
    faces = face_detection_blend(im)

    if len(faces) == 0:
        return None

    faceBoxes = {k : {"points" : None,
                      "shape" : None,
                      "face" : None} for k in range(len(faces))}
    for i, bbox in enumerate(faces):
        points = np.asarray(face_points_detection(predictor, im, bbox))

        im_w, im_h = im.shape[:2]
        left, top = np.min(points, 0)
        right, bottom = np.max(points, 0)

        x, y = max(0, left - r), max(0, top - r)
        w, h = min(right + r, im_h) - x, min(bottom + r, im_w) - y
        faceBoxes[i]["points"] = points - np.asarray([[x, y]])
        faceBoxes[i]["shape"] = (x, y, w, h)
        faceBoxes[i]["face"] = im[y:y + h, x:x + w]

    return faceBoxes

## 3D Transform
def bilinear_interpolate(img, coords):
    """ Interpolates over every image channel
    http://en.wikipedia.org/wiki/Bilinear_interpolation
    :param img: max 3 channel image
    :param coords: 2 x _m_ array. 1st row = xcoords, 2nd row = ycoords
    :returns: array of interpolated pixels with same shape as coords
    """
    int_coords = np.int32(coords)
    x0, y0 = int_coords
    dx, dy = coords - int_coords

    # 4 Neighour pixels
    q11 = img[y0, x0]
    q21 = img[y0, x0 + 1]
    q12 = img[y0 + 1, x0]
    q22 = img[y0 + 1, x0 + 1]

    btm = q21.T * dx + q11.T * (1 - dx)
    top = q22.T * dx + q12.T * (1 - dx)
    inter_pixel = top * dy + btm * (1 - dy)

    return inter_pixel.T

def grid_coordinates(points):
    """ x,y grid coordinates within the ROI of supplied points
    :param points: points to generate grid coordinates
    :returns: array of (x, y) coordinates
    """
    xmin = np.min(points[:, 0])
    xmax = np.max(points[:, 0]) + 1
    ymin = np.min(points[:, 1])
    ymax = np.max(points[:, 1]) + 1

    return np.asarray([(x, y) for y in range(ymin, ymax)
                       for x in range(xmin, xmax)], np.uint32)

def process_warp(src_img, result_img, tri_affines, dst_points, delaunay):
    """
    Warp each triangle from the src_image only within the
    ROI of the destination image (points in dst_points).
    """
    roi_coords = grid_coordinates(dst_points)
    # indices to vertices. -1 if pixel is not in any triangle
    roi_tri_indices = delaunay.find_simplex(roi_coords)

    for simplex_index in range(len(delaunay.simplices)):
        coords = roi_coords[roi_tri_indices == simplex_index]
        num_coords = len(coords)
        out_coords = np.dot(tri_affines[simplex_index],
                            np.vstack((coords.T, np.ones(num_coords))))
        x, y = coords.T
        result_img[y, x] = bilinear_interpolate(src_img, out_coords)

    return None

def triangular_affine_matrices(vertices, src_points, dst_points):
    """
    Calculate the affine transformation matrix for each
    triangle (x,y) vertex from dst_points to src_points
    :param vertices: array of triplet indices to corners of triangle
    :param src_points: array of [x, y] points to landmarks for source image
    :param dst_points: array of [x, y] points to landmarks for destination image
    :returns: 2 x 3 affine matrix transformation for a triangle
    """
    ones = [1, 1, 1]
    for tri_indices in vertices:
        src_tri = np.vstack((src_points[tri_indices, :].T, ones))
        dst_tri = np.vstack((dst_points[tri_indices, :].T, ones))
        mat = np.dot(src_tri, np.linalg.inv(dst_tri))[:2, :]
        yield mat

def warp_image_3d(src_img, src_points, dst_points, dst_shape, dtype=np.uint8):
    rows, cols = dst_shape[:2]
    result_img = np.zeros((rows, cols, 3), dtype=dtype)

    delaunay = spatial.Delaunay(dst_points)
    tri_affines = np.asarray(list(triangular_affine_matrices(
        delaunay.simplices, src_points, dst_points)))

    process_warp(src_img, result_img, tri_affines, dst_points, delaunay)

    return result_img

## 2D Transform
def transformation_from_points(points1, points2):
    points1 = points1.astype(np.float64)
    points2 = points2.astype(np.float64)

    c1 = np.mean(points1, axis=0)
    c2 = np.mean(points2, axis=0)
    points1 -= c1
    points2 -= c2

    s1 = np.std(points1)
    s2 = np.std(points2)
    points1 /= s1
    points2 /= s2

    U, S, Vt = np.linalg.svd(np.dot(points1.T, points2))
    R = (np.dot(U, Vt)).T

    return np.vstack([np.hstack([s2 / s1 * R,
                                (c2.T - np.dot(s2 / s1 * R, c1.T))[:, np.newaxis]]),
                      np.array([[0., 0., 1.]])])

def warp_image_2d(im, M, dshape):
    output_im = np.zeros(dshape, dtype=im.dtype)
    cv2.warpAffine(im,
                   M[:2],
                   (dshape[1], dshape[0]),
                   dst=output_im,
                   borderMode=cv2.BORDER_TRANSPARENT,
                   flags=cv2.WARP_INVERSE_MAP)

    return output_im

## Generate Mask
def mask_from_points(size, points,erode_flag=1):
    radius = 10  # kernel size
    kernel = np.ones((radius, radius), np.uint8)

    mask = np.zeros(size, np.uint8)
    cv2.fillConvexPoly(mask, cv2.convexHull(points), 255)
    if erode_flag:
        mask = cv2.erode(mask, kernel,iterations=1)

    return mask

## Color Correction
def correct_colours(im1, im2, landmarks1):
    COLOUR_CORRECT_BLUR_FRAC = 0.75
    LEFT_EYE_POINTS = list(range(42, 48))
    RIGHT_EYE_POINTS = list(range(36, 42))

    blur_amount = COLOUR_CORRECT_BLUR_FRAC * np.linalg.norm(
                              np.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -
                              np.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))
    blur_amount = int(blur_amount)
    if blur_amount % 2 == 0:
        blur_amount += 1
    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)
    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)

    # Avoid divide-by-zero errors.
    im2_blur = im2_blur.astype(int)
    im2_blur += 128*(im2_blur <= 1)

    result = im2.astype(np.float64) * im1_blur.astype(np.float64) / im2_blur.astype(np.float64)
    result = np.clip(result, 0, 255).astype(np.uint8)

    return result

## Copy-and-paste
def apply_mask(img, mask):
    """ Apply mask to supplied image
    :param img: max 3 channel image
    :param mask: [0-255] values in mask
    :returns: new image with mask applied
    """
    masked_img=cv2.bitwise_and(img,img,mask=mask)

    return masked_img


## Alpha blending
def alpha_feathering(src_img, dest_img, img_mask, blur_radius=15):
    mask = cv2.blur(img_mask, (blur_radius, blur_radius))
    mask = mask / 255.0

    result_img = np.empty(src_img.shape, np.uint8)
    for i in range(3):
        result_img[..., i] = src_img[..., i] * mask + dest_img[..., i] * (1-mask)

    return result_img


def check_points(img,points):
    # Todo: I just consider one situation.
    if points[8,1]>img.shape[0]:
        logging.error("Jaw part out of image")
    else:
        return True
    return False

## Face detection
def face_detection_blend(img,upsample_times=1):
    # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    detector = dlib.get_frontal_face_detector()
    faces = detector(img, upsample_times)

    return faces

def face_swap(src_face, dst_face, src_points, dst_points, dst_shape, dst_img, end=48): #args, end=48):
    h, w = dst_face.shape[:2]

    ## 3d warp
    warped_src_face = warp_image_3d(src_face, src_points[:end], dst_points[:end], (h, w))
    ## Mask for blending
    mask = mask_from_points((h, w), dst_points)
    mask_src = np.mean(warped_src_face, axis=2) > 0
    mask = np.asarray(mask * mask_src, dtype=np.uint8)
    ## Correct color
    if True:#args.correct_color:
        warped_src_face = apply_mask(warped_src_face, mask)
        dst_face_masked = apply_mask(dst_face, mask)
        warped_src_face = correct_colours(dst_face_masked, warped_src_face, dst_points)
    ## 2d warp
    if False:#args.warp_2d:
        unwarped_src_face = warp_image_3d(warped_src_face, dst_points[:end], src_points[:end], src_face.shape[:2])
        warped_src_face = warp_image_2d(unwarped_src_face, transformation_from_points(dst_points, src_points),
                                        (h, w, 3))

        mask = mask_from_points((h, w), dst_points)
        mask_src = np.mean(warped_src_face, axis=2) > 0
        mask = np.asarray(mask * mask_src, dtype=np.uint8)

    ## Shrink the mask
    kernel = np.ones((10, 10), np.uint8)
    mask = cv2.erode(mask, kernel, iterations=1)
    ##Poisson Blending
    r = cv2.boundingRect(mask)
    center = ((r[0] + int(r[2] / 2), r[1] + int(r[3] / 2)))
    output = cv2.seamlessClone(warped_src_face, dst_face, mask, center, cv2.NORMAL_CLONE)

    x, y, w, h = dst_shape
    dst_img_cp = dst_img.copy()
    dst_img_cp[y:y + h, x:x + w] = output

    return dst_img_cp

def blend_faces():
  print("todo")
  PREDICTOR_PATH = 'landmark_model.dat'
  predictor = dlib.shape_predictor(PREDICTOR_PATH)

  src_img = cv2.imread('test_images/d.jpg')
  src_points, src_shape, src_face = select_face(predictor, src_img)
  dst_img = cv2.imread('test_images/Unknown-8.jpg')
  

   # Select src face
  src_points, src_shape, src_face = select_face(predictor, src_img)
  # Select dst face
  dst_faceBoxes = select_all_faces(predictor, dst_img)

  if dst_faceBoxes is None:
    print('Detect 0 Face !!!')

  output = dst_img
  for k, dst_face in dst_faceBoxes.items():
    output = face_swap(src_face, dst_face["face"], src_points,
      dst_face["points"], dst_face["shape"],
        output)

  cv2_imshow(src_img)
  cv2_imshow(dst_img)
  cv2_imshow(output)

resultImg, faceBoxes = detect_faces(picture1)
if not faceBoxes:
  print("No face detected")

else:
  classified_faces = classify_faces(picture1, faceBoxes, resultImg)
  print(classified_faces)
  blur_faces(picture1, faceBoxes)
  for face in classified_faces:
    generate_faces(gender = face[0], age = face[1])
  detect_background("test_images/e.JPG")
  blend_faces()



